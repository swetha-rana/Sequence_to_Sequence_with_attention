{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Installing Wandb"
      ],
      "metadata": {
        "id": "I6m-eXcjw7nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNo1Rx62t-Ca",
        "outputId": "51b7607a-f2c0-46f7-e590-37ed43fe635c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Pacakages**"
      ],
      "metadata": {
        "id": "MoqmFoDoxW4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t_XSw-pFIqmh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib.font_manager import FontProperties\n",
        "%matplotlib inline\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the Dataset"
      ],
      "metadata": {
        "id": "Oa5VedZDxgtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "!tar -xvf  'daksh.tar'"
      ],
      "metadata": {
        "id": "APJEXjggI3tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Lexicons from the Tamil Dataset**"
      ],
      "metadata": {
        "id": "R77lylLSxn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training lexicons\n",
        "with open(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\") as f:\n",
        "    train_lines = f.readlines()\n",
        "train_pairs = [line.strip(\"\\n\").split(\"\\t\") for line in train_lines]\n",
        "train_map = dict([(example[1], example[0]) for example in train_pairs])\n",
        "\n",
        "# Load the validation lexicons\n",
        "with open(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\") as f:\n",
        "    validation_lines = f.readlines()\n",
        "validation_pairs = [line.strip(\"\\n\").split(\"\\t\") for line in validation_lines]\n",
        "validation_map = dict([(example[1], example[0]) for example in validation_pairs])\n",
        "\n",
        "# Load the test lexicons\n",
        "with open(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\") as f:\n",
        "    test_lines = f.readlines()\n",
        "test_pairs = [line.strip(\"\\n\").split(\"\\t\") for line in test_lines]\n",
        "test_map = dict([(example[1], example[0]) for example in test_pairs])\n",
        "\n",
        "# Number of training examples\n",
        "M_train = len(train_map.keys())\n",
        "\n",
        "# Number of validation examples\n",
        "M_val = len(validation_map.keys())\n",
        "\n",
        "# Number of test examples\n",
        "M_test = len(test_map.keys())\n"
      ],
      "metadata": {
        "id": "HRrlF2ITJCTi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Dataset for building the model\n",
        "\n",
        "Below Cell Contains Two functions\n",
        "\n",
        "**Function Name**: data\n",
        "\n",
        "**Description** : This function responsible for data preprocessing, augmentation.\n",
        "\n",
        "**Arguments** : data_dict\n",
        "\n",
        "**Returns** : input_characters, target_characters, num_input_tokens, num_target_tokens, max_encoder_seq_length, max_decoder_seq_length, input_char_map, target_char_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Function Name**: one_hot_coding.\n",
        "\n",
        "\n",
        "**Description** : This also like preproccesing which wll help to convert the array as one hot encoding\n",
        "\n",
        "**Arguments** : data_dict, max_encoder_seq_length, max_decoder_seq_length, num_input_tokens, num_target_tokens\n",
        "\n",
        "**Returns**: input_words, target_words, encoder_input_data, decoder_input_data, decoder_output_data."
      ],
      "metadata": {
        "id": "h3tHd_INx0vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocess(data_dict):\n",
        "    \"\"\"\n",
        "    Returns important information about the data.\n",
        "    \"\"\"\n",
        "    input_words = []\n",
        "    target_words = []\n",
        "    input_characters = []\n",
        "    target_characters = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        # Store the word in the source language\n",
        "        input_words.append(key)\n",
        "        # Store the word in the target language\n",
        "        target_words.append(\"\\t\"+data_dict[key]+\"\\n\")\n",
        "        # Add the characters to the respective character lists\n",
        "        input_characters = list(set(input_characters + list(key)))\n",
        "        target_characters = list(set(target_characters + list(data_dict[key])))\n",
        "\n",
        "    # Sort the input characters\n",
        "    input_characters = sorted(list(set(input_characters)))\n",
        "    # Sort the target characters\n",
        "    target_characters = target_characters + [\"\\t\", \"\\n\"]\n",
        "    target_characters = sorted(list(set(target_characters)))\n",
        "\n",
        "    # Number of unique tokens in the source language\n",
        "    num_input_tokens = len(input_characters)\n",
        "    # Number of unique tokens in the target language\n",
        "    num_target_tokens = len(target_characters)\n",
        "\n",
        "    # Max input word length\n",
        "    max_encoder_seq_length = max([len(word) for word in input_words]) \n",
        "    # Max output word length\n",
        "    max_decoder_seq_length = max([len(word) for word in target_words])\n",
        "    # Map characters to numerical indices (using +1 to avoid any character being mapped to 0)\n",
        "    input_char_map = dict([(ch, i + 1) for i, ch in enumerate(input_characters)])\n",
        "    target_char_map = dict([(ch, i + 1) for i, ch in enumerate(target_characters)])\n",
        "\n",
        "    return input_characters, target_characters, num_input_tokens, num_target_tokens, max_encoder_seq_length, max_decoder_seq_length, input_char_map, target_char_map\n",
        "\n",
        "def one_hot_coding(data_dict, max_encoder_seq_length, max_decoder_seq_length, num_input_tokens, num_target_tokens):\n",
        "    \"\"\"\n",
        "    This function takes the training/validation/test dictionary as input and produces\n",
        "    the one-hot encoded versions of the respective data.\n",
        "    \"\"\"\n",
        "    input_words = []\n",
        "    target_words = []\n",
        "\n",
        "    for key in data_dict:\n",
        "        # Store the word in the source language\n",
        "        input_words.append(key)\n",
        "        # Store the word in the target language\n",
        "        target_words.append(\"\\t\"+data_dict[key]+\"\\n\")\n",
        "\n",
        "    M = len(data_dict.keys())\n",
        "    encoder_input_data = np.zeros((M, max_encoder_seq_length, num_input_tokens + 1), dtype=\"float\")\n",
        "    decoder_input_data = np.zeros((M, max_decoder_seq_length, num_target_tokens + 1), dtype=\"float\")\n",
        "    decoder_output_data = np.zeros((M, max_decoder_seq_length, num_target_tokens + 1), dtype=\"float\")\n",
        "\n",
        "    for i in range(M):\n",
        "        source_word = input_words[i]\n",
        "        target_word = target_words[i]\n",
        "\n",
        "        # One-hot encoding for the input\n",
        "        for j, ch in enumerate(source_word):\n",
        "            encoder_input_data[i, j, input_char_map[ch]] = 1.0\n",
        "\n",
        "        # One-hot encoding for the output\n",
        "        for j, ch in enumerate(target_word):\n",
        "            decoder_input_data[i, j, target_char_map[ch]]= 1.0\n",
        "            if j >= 1:\n",
        "                # The decoder output is one step ahead of the decoder input\n",
        "                decoder_output_data[i, j-1, target_char_map[ch]] = 1.0\n",
        "\n",
        "    \n",
        "\n",
        "    return input_words, target_words, encoder_input_data, decoder_input_data, decoder_output_data\n"
      ],
      "metadata": {
        "id": "PFIi68pYJ2mX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_characters, target_characters, num_input_tokens, num_target_tokens, max_encoder_seq_length, max_decoder_seq_length, input_char_map, target_char_map = data_preprocess(train_map)\n",
        "input_words, target_words, encoder_input_data, decoder_input_data, decoder_output_data = one_hot_coding(train_map, max_encoder_seq_length, max_decoder_seq_length, num_input_tokens, num_target_tokens)\n",
        "val_input_words, val_target_words, val_encoder_input_data, val_decoder_input_data, val_decoder_output_data = one_hot_coding(validation_map, max_encoder_seq_length, max_decoder_seq_length, num_input_tokens, num_target_tokens)\n",
        "test_input_words, test_target_words, test_encoder_input_data, test_decoder_input_data, test_decoder_output_data = one_hot_coding(test_map, max_encoder_seq_length, max_decoder_seq_length, num_input_tokens, num_target_tokens)"
      ],
      "metadata": {
        "id": "n4RGEKmJJ5kM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using label encoding for the encoder inputs (and then find an embedding using the Embedding layer)\n",
        "encoder_input_data = np.argmax(encoder_input_data, axis=2)\n",
        "val_encoder_input_data = np.argmax(val_encoder_input_data, axis=2)\n",
        "test_encoder_input_data = np.argmax(test_encoder_input_data, axis=2)\n",
        "\n",
        "decoder_input_data = np.argmax(decoder_input_data, axis=2)\n",
        "val_decoder_input_data = np.argmax(val_decoder_input_data, axis=2)\n",
        "test_decoder_input_data = np.argmax(test_decoder_input_data, axis=2)\n",
        "\n",
        "# Dictionaries mapping from indices to characters\n",
        "reverse_input_char_map = dict((i, char) for char, i in input_char_map.items())\n",
        "reverse_target_char_map = dict((i, char) for char, i in target_char_map.items())\n",
        "reverse_target_char_map[0] = \"\\n\""
      ],
      "metadata": {
        "id": "5lOm4kroKDyu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Attention Layer**"
      ],
      "metadata": {
        "id": "YGk00IAN2i3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "ktwL0jxwKFBz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Rnn Model with attention**\n",
        "\n",
        "Below Cell Contains a function of Build_model\n",
        "\n",
        "**Function Name**: build_model\n",
        "\n",
        "**Description** : This function responsible for building the rnn network with attention\n",
        "\n",
        "**Arguments** : latent_dim,rnn_type,embedding_dim,dropout\n",
        "\n",
        "**Returns** : Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6d3Gpt1H3EIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(latent_dim, rnn_type, embedding_dim, dropout):\n",
        "    \"\"\"\n",
        "    latent_dim: Number of cells in the encoder and decoder layers\n",
        "    rnn_type: choice of cell type: Simple RNN, LSTM, GRU\n",
        "    num_encoder_layers: Number of layers in the encoder\n",
        "    num_decoder_layers: Number of layers in the decoder\n",
        "    embedding_dim: Dimenions of the vector to represent each character\n",
        "    dropout: fraction of neurons to drop out\n",
        "    \"\"\"\n",
        "    ## ENCODER\n",
        "    # encoder_input = keras.Input(shape=(None, num_input_tokens), name=\"EncoderInput\")\n",
        "    encoder_input = keras.Input(shape=(None, ), name=\"EncoderInput\")\n",
        "    encoder_embedding = keras.layers.Embedding(num_input_tokens + 1, embedding_dim, name=\"EncoderInputEmbedding\", mask_zero=True)(encoder_input)\n",
        "\n",
        "    #encoder lstm 1\n",
        "    if rnn_type == 'LSTM':\n",
        "        encoder_lstm = tf.keras.layers.LSTM(latent_dim,return_sequences=True,return_state=True,name=\"EncoderLayer\" ,dropout=dropout,recurrent_dropout=dropout)\n",
        "        encoder_output, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    if rnn_type == 'GRU':\n",
        "        encoder_gru = tf.keras.layers.GRU(latent_dim,return_sequences=True,return_state=True,name=\"EncoderLayer\" ,dropout=dropout,recurrent_dropout=dropout)\n",
        "        encoder_output, gru_state = encoder_gru(encoder_embedding)\n",
        "    if rnn_type == 'RNN':\n",
        "        encoder_rnn = tf.keras.layers.SimpleRNN(latent_dim,return_sequences=True,return_state=True,name=\"EncoderLayer\" ,dropout=dropout,recurrent_dropout=dropout)\n",
        "        encoder_output, rnn_state = encoder_rnn(encoder_embedding)   \n",
        "\n",
        "    ## DECODER\n",
        "    decoder_input = keras.Input(shape=(None, ), name=\"DecoderInput\")\n",
        "    dec_emb = keras.layers.Embedding(num_target_tokens + 1, 64, name=\"DecoderInputEmbedding\", mask_zero=True)(decoder_input)\n",
        "\n",
        "    if rnn_type == 'LSTM':\n",
        "        decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, name=\"DecoderLayer\" ,dropout=dropout,recurrent_dropout=dropout )\n",
        "        decoder_output,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "    if rnn_type == 'GRU':\n",
        "        decoder_gru = tf.keras.layers.GRU(latent_dim,return_sequences=True,return_state=True,name=\"DecoderLayer\" ,dropout=dropout,recurrent_dropout=dropout)\n",
        "        decoder_output, decoder_state_gru = decoder_gru(dec_emb,initial_state = gru_state)\n",
        "    if rnn_type =='RNN':\n",
        "        decoder_rnn = tf.keras.layers.SimpleRNN(latent_dim,return_sequences=True,return_state=True,name=\"DecoderLayer\" ,dropout=dropout,recurrent_dropout=dropout)\n",
        "        decoder_output, decoder_state_rnn = decoder_rnn(dec_emb,initial_state = rnn_state)\n",
        "\n",
        "    # Attention layer\n",
        "    attn_out, attn_states = AttentionLayer(name='attention_layer')([encoder_output, decoder_output])\n",
        "\n",
        "\n",
        "    # Concat attention input and decoder LSTM output\n",
        "    decoder_concat_input = tf.keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_output, attn_out])\n",
        "\n",
        "    #dense layer\n",
        "    decoder_dense =  tf.keras.layers.TimeDistributed(keras.layers.Dense(num_target_tokens + 1, activation='softmax'))\n",
        "    #decoder_dense = keras.layers.Dense(num_target_tokens + 1, activation=\"softmax\", name=\"DecoderDenseLayer\") # Softmax picks one character\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "    # Define the model \n",
        "    model = keras.Model([encoder_input, decoder_input], decoder_outputs)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "c3zjrmFOKNSa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Inference Model with attention**\n",
        "\n",
        "Below Cell Contains a function of Build_Inference_model\n",
        "\n",
        "**Function Name**:build_inference_model\n",
        "\n",
        "**Description** : This function responsible for building the inference model.\n",
        "\n",
        "**Arguments** : model,rnn_type, latent_dim\n",
        "\n",
        "**Returns** : encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "0XsOTag339Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inference_model(model,rnn_type, latent_dim):\n",
        "  if rnn_type == \"LSTM\":\n",
        "              # Input to the encoder, sequence of characters (word) in the source language\n",
        "              encoder_inputs = model.input[0]\n",
        "\n",
        "              # Output of the encoder\n",
        "              encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output\n",
        "              encoder_states = [encoder_outputs,state_h_enc, state_c_enc]\n",
        "\n",
        "              # Create an encoder model \n",
        "              encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "              # Input to the decoder\n",
        "\n",
        "              decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "              decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "              decoder_hidden_state_inputs = keras.Input(shape=(None,latent_dim))\n",
        "              decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "              decoder_inputs = model.layers[1].output\n",
        "              decoder_embedding_layer = model.layers[3]\n",
        "              decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "              decoder_lstm = model.layers[5]\n",
        "\n",
        "              decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "              decoder_states = [state_h_dec, state_c_dec]\n",
        "\n",
        "              #attention\n",
        "              attn_layer = model.layers[6]\n",
        "              attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_inputs,decoder_outputs])\n",
        "\n",
        "              #concat\n",
        "              concate = model.layers[7]\n",
        "              decoder_inf_concat = concate([decoder_outputs,attn_out_inf])\n",
        "\n",
        "              # Softmax layer\n",
        "              decoder_dense = model.layers[8]\n",
        "              decoder_outputs = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "              # Create the decoder model\n",
        "              decoder_model = keras.Model([decoder_inputs] + [decoder_hidden_state_inputs,decoder_state_input_h,decoder_state_input_c], [decoder_outputs] + decoder_states +[attn_states_inf])\n",
        "\n",
        "\n",
        "  else:\n",
        "              # Input to the encoder, sequence of characters (word) in the source language\n",
        "              encoder_inputs = model.input[0]\n",
        "\n",
        "              # Output of the encoder\n",
        "              encoder_outputs, state_gru_enc = model.layers[4].output\n",
        "              encoder_states = [encoder_outputs,state_gru_enc]\n",
        "\n",
        "              # Create an encoder model \n",
        "              encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "              # Input to the decoder\n",
        "\n",
        "              decoder_state_input = keras.Input(shape=(latent_dim,))\n",
        "              decoder_hidden_state_inputs = keras.Input(shape=(None,latent_dim))\n",
        "              decoder_states_inputs = [decoder_state_input]\n",
        "\n",
        "              decoder_inputs = model.layers[1].output\n",
        "              decoder_embedding_layer = model.layers[3]\n",
        "              decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "              decoder_gru = model.layers[5]\n",
        "\n",
        "              decoder_outputs, state_gru_dec = decoder_gru(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "              decoder_states = [state_gru_dec]\n",
        "\n",
        "\n",
        "              #attention\n",
        "              attn_layer = model.layers[6]\n",
        "              attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_inputs,decoder_outputs])\n",
        "\n",
        "              #concat\n",
        "              concate = model.layers[7]\n",
        "              decoder_inf_concat = concate([decoder_outputs,attn_out_inf])\n",
        "\n",
        "              # Softmax layer\n",
        "              decoder_dense = model.layers[8]\n",
        "              decoder_outputs = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "              # Create the decoder model\n",
        "              decoder_model = keras.Model([decoder_inputs] + [decoder_hidden_state_inputs,decoder_state_input], [decoder_outputs] + decoder_states +[attn_states_inf])\n",
        "\n",
        "  return encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "t8J_BE9HKP8P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding the Sequence**\n",
        "\n",
        "Below Cell Contains a function of decode_sequence\n",
        "\n",
        "**Function Name**:decode_sequence\n",
        "\n",
        "**Description** : This function responsible for decoding the words\n",
        "\n",
        "**Arguments** : input_words,rnn_type ,encoder_model, decoder_model\n",
        "\n",
        "**Returns** : decoded_words\n"
      ],
      "metadata": {
        "id": "cTczS_J-4lF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def decode_sequence(input_words,rnn_type ,encoder_model, decoder_model):\n",
        "\n",
        "    \"\"\"\n",
        "    Decodes the given input sequence, one character at a time.\n",
        "    \"\"\"\n",
        "    if rnn_type == \"LSTM\":\n",
        "                # Get batch_size\n",
        "                batch_size = input_words.shape[0]\n",
        "                # Encode the input string\n",
        "                encoded_hidden_cell_states = encoder_model.predict(input_words)\n",
        "\n",
        "                target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "                # Set the first character to \"tab\" as it is the start of sequence character\n",
        "                target_sequence[:, 0, target_char_map[\"\\t\"]] = 1.0\n",
        "                target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "                decoded_words = [\"\"]*batch_size\n",
        "                for i in range(max_decoder_seq_length):\n",
        "                    output_tokens, h, c,attn = decoder_model.predict([target_sequence] + encoded_hidden_cell_states)\n",
        "\n",
        "                    # Sample the most probable character using softmax outputs\n",
        "                    sampled_char_indices = np.argmax(output_tokens[:, -1, :], axis=1)\n",
        "\n",
        "                    # Update the target sequence which goes back as input to the decoder.\n",
        "                    target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "\n",
        "                    for j, ch_index in enumerate(sampled_char_indices):\n",
        "                        decoded_words[j] += reverse_target_char_map[ch_index]\n",
        "                        target_sequence[j, 0, ch_index] = 1.0\n",
        "\n",
        "                    target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "                    # Update the hidden state and cell state \n",
        "                    encoded_hidden_cell_states[1],encoded_hidden_cell_states[2] = [h, c]\n",
        "\n",
        "                # To remove the \\n chars\n",
        "                decoded_words = [word[:word.find(\"\\n\")] for word in decoded_words]\n",
        "    else:\n",
        "                            # Get batch_size\n",
        "                batch_size = input_words.shape[0]\n",
        "                # Encode the input string\n",
        "                encoded_hidden_cell_states = encoder_model.predict(input_words)\n",
        "\n",
        "                target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "                # Set the first character to \"tab\" as it is the start of sequence character\n",
        "                target_sequence[:, 0, target_char_map[\"\\t\"]] = 1.0\n",
        "                target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "                decoded_words = [\"\"]*batch_size\n",
        "                for i in range(max_decoder_seq_length):\n",
        "                    output_tokens, state, attn = decoder_model.predict([target_sequence] + [encoded_hidden_cell_states])\n",
        "\n",
        "                    # Sample the most probable character using softmax outputs\n",
        "                    sampled_char_indices = np.argmax(output_tokens[:, -1, :], axis=1)\n",
        "\n",
        "                    # Update the target sequence which goes back as input to the decoder.\n",
        "                    target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "\n",
        "                    for j, ch_index in enumerate(sampled_char_indices):\n",
        "                        decoded_words[j] += reverse_target_char_map[ch_index]\n",
        "                        target_sequence[j, 0, ch_index] = 1.0\n",
        "\n",
        "                    target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "                    # Update the hidden state and cell state \n",
        "                    encoded_hidden_cell_states[1] = [state]\n",
        "\n",
        "                # To remove the \\n chars\n",
        "                decoded_words = [word[:word.find(\"\\n\")] for word in decoded_words]\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "vzlKEs-sqvOT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Below Cell Contains Function name called setRunName.\n",
        "\n",
        "**Function Name**: setRunName\n",
        "\n",
        "**Description** : This function responsible for creating the run name based on sweep config.\n",
        "\n",
        "**Arguments** : rnn_type,latent_dim,embedding_dim,batch_size,epochs\n",
        "Returns : run name.\n"
      ],
      "metadata": {
        "id": "Kv5FS1LG5UYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_run_name(rnn_type,latent_dim,embedding_dim,batch_size,epochs):         \n",
        "         \n",
        "         run_name = \"_\".join([\"cell\",rnn_type,\"ncell\",str(latent_dim),\"emb\", str(embedding_dim), \"dp\", str(dropout), \"bs\", str(batch_size),\n",
        "                      \"epoc\", str(epochs)])\n",
        "    \n",
        "         return run_name"
      ],
      "metadata": {
        "id": "nSnga7TcrtLg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell contains the train function\n",
        "\n",
        "**Function name** : train\n",
        "\n",
        "**Description** : This function contains the config defaluts and the code for building the rnn model. We call this function from wand.agent().\n",
        "\n",
        "**Arguments** : None\n",
        "\n",
        "**Return** : None"
      ],
      "metadata": {
        "id": "7a5bpU0n5jW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    \"\"\"\n",
        "    This function performs hyperparameter search using WANDB\n",
        "    \"\"\"\n",
        "    #, , num_encoder_layers, num_decoder_layers, embedding_dim, dropout, \n",
        "\n",
        "    # Default values for hyper-parameters\n",
        "    config_defaults = {\n",
        "        \"latent_dim\": 256,\n",
        "        \"rnn_type\": \"LSTM\",\n",
        "        \"embedding_dim\": 16,\n",
        "        \"batch_size\": 64,\n",
        "        \"epochs\": 10\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    # Local variables, values obtained from wandb config\n",
        "    rnn_type = config.rnn_type\n",
        "    latent_dim = config.latent_dim\n",
        "    embedding_dim = config.embedding_dim\n",
        "    dropout = 0.3\n",
        "    batch_size = config.batch_size\n",
        "    epochs = config.epochs\n",
        "\n",
        "    # Define the model\n",
        "    model = build_model(latent_dim, rnn_type, embedding_dim, dropout, 1)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "            [encoder_input_data, decoder_input_data],\n",
        "            decoder_output_data,\n",
        "            batch_size = batch_size,\n",
        "            epochs = epochs,\n",
        "            verbose = 2,\n",
        "            validation_data = ([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "            callbacks=[WandbCallback()]\n",
        "            )\n",
        "    \n",
        "    \n",
        "\n",
        "    # Get the encoder and decoder model\n",
        "    encoder_model, decoder_model = build_inference_model(model,config.rnn_type,config.latent_dim)\n",
        "\n",
        "    outputs = []\n",
        "    n = val_encoder_input_data.shape[0]\n",
        "    batch_size = 1000\n",
        "    for i in range(0, n, batch_size):\n",
        "        # Inputs\n",
        "        query = val_encoder_input_data[i:i+batch_size]\n",
        "        # Results\n",
        "        decoded_words = decode_sequence(query,config.rnn_type,encoder_model, decoder_model)\n",
        "        outputs = outputs + decoded_words\n",
        "\n",
        "    # To remove the tab and newline characters from the ground truth\n",
        "    actual_words = [word[1:-1] for word in val_target_words]\n",
        "    # Calculate validation accuracy\n",
        "    validation_inference_accuracy = np.mean(np.array(outputs) == np.array(actual_words))\n",
        "    print(\"Validation accuracy based on whole string matching = {} %\".format(validation_inference_accuracy*100.0))\n",
        "\n",
        "    wandb.log({\"inference_val_accuracy\": validation_inference_accuracy})\n",
        "\n",
        "    # Plots of accuracy and loss\n",
        "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
        "    plt.plot(history.history[\"accuracy\"], label = \"Training accuracy\")\n",
        "    plt.title(\"Accuracy vs epoch\", size=14)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "    plt.plot(history.history[\"loss\"], label = \"Training loss\")\n",
        "    plt.title(\"Loss vs epoch\", size=14)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Meaningful name for the run\n",
        "    wandb.run.name = set_run_name(config.rnn_type,config.latent_dim,config.embedding_dim,config.batch_size,config.epochs)\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()"
      ],
      "metadata": {
        "id": "UaShNMAVKXSq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Below cell contains the wandb sweep config.\n"
      ],
      "metadata": {
        "id": "Yqrv8Cjf58sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "  \"name\": \"Assignment 3\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"bayes\",\n",
        "  \"parameters\": {\n",
        "        \"rnn_type\": {\n",
        "            \"values\": [\"LSTM\", \"RNN\", \"GRU\"]\n",
        "        },\n",
        "        \"latent_dim\": {\n",
        "            \"values\": [128, 256, 512]\n",
        "        },\n",
        "        \"embedding_dim\": {\n",
        "            \"values\": [126, 258, 64]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 64, 128]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [10]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "BECKxyn7KXNJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = \"ou020ze8\"\n",
        "wandb.agent(sweep_id, train,entity=\"swe-rana\", project=\"CS6910_Assignment3\", count=20)"
      ],
      "metadata": {
        "id": "VlA6X4UfKdfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}